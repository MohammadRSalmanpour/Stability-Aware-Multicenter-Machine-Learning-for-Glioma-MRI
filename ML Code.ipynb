{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0LMpX-ma77H3"},"outputs":[],"source":["\n","\n","\n","# =============================================================================\n","# Automatic Installation of Required Packages\n","# =============================================================================\n","import subprocess  # Module to run subprocess commands (e.g., pip installation)\n","import sys         # System-specific parameters and functions\n","\n","# A list of required packages that will be installed if not already present.\n","required_packages = [\n","    'pandas', 'numpy', 'matplotlib', 'seaborn', 'scikit-learn', 'xgboost',\n","    'lightgbm', 'openpyxl', 'tensorflow', 'imblearn',\n","    'statsmodels', 'skrebate', 'umap-learn', 'boruta', 'shap', 'joblib', 'xlsxwriter'\n","]\n","\n","# Loop through each package and try to import it; if the package is missing, install it.\n","for package in required_packages:\n","    try:\n","        # Replace hyphen with underscore when importing (if needed)\n","        __import__(package.replace('-', '_'))\n","    except ImportError:\n","        # Install the missing package using pip\n","        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n","\n","# =============================================================================\n","# Importing Required Libraries\n","# =============================================================================\n","import os                          # For file and path operations\n","import shutil                      # For file operations (e.g., copying files)\n","import logging                     # For logging error and info messages\n","from pathlib import Path           # For object-oriented filesystem paths\n","from datetime import datetime      # For handling date and time\n","\n","import pandas as pd                # Data manipulation and analysis\n","import numpy as np                 # Numerical operations\n","\n","import matplotlib.pyplot as plt    # Plotting library\n","import seaborn as sns              # Statistical data visualization\n","import traceback\n","# Importing specific models and functions from scikit-learn and other libraries\n","\n","# Gaussian Process Classifier and Multi-layer Perceptron classifier from sklearn\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.neural_network import MLPClassifier\n","\n","# Model selection and cross-validation tools\n","from sklearn.model_selection import (train_test_split, StratifiedKFold,\n","                                     RandomizedSearchCV, GridSearchCV)\n","\n","# Metrics for evaluation\n","from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n","                             roc_auc_score, confusion_matrix)\n","\n","# Imputation and scaling/preprocessing functions\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import (MinMaxScaler, StandardScaler, RobustScaler, Normalizer, MaxAbsScaler)\n","\n","# Feature selection functions and methods\n","from sklearn.feature_selection import (SelectKBest, chi2, f_classif, mutual_info_classif,\n","                                       VarianceThreshold, RFE, SequentialFeatureSelector, SelectFdr,\n","                                       SelectFwe)\n","\n","# Dimensionality reduction methods (feature extraction)\n","from sklearn.decomposition import (PCA, FastICA, TruncatedSVD, KernelPCA, FactorAnalysis, SparsePCA, NMF)\n","\n","# Clustering and hierarchical grouping for features\n","from sklearn.cluster import FeatureAgglomeration\n","\n","# Manifold learning techniques for non-linear dimension reduction\n","from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding, MDS, SpectralEmbedding\n","\n","# Importing classifiers from XGBoost and LightGBM libraries\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","\n","# Importing additional classifiers from scikit-learn\n","from sklearn.svm import SVC\n","from sklearn.linear_model import (LogisticRegression, Lasso, RidgeClassifier, ElasticNet,\n","                                  LogisticRegressionCV, SGDClassifier)\n","from sklearn.naive_bayes import GaussianNB, BernoulliNB, ComplementNB\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n","from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, VotingClassifier,\n","                              GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier,\n","                              HistGradientBoostingClassifier, StackingClassifier)\n","from sklearn.calibration import CalibratedClassifierCV\n","from sklearn.dummy import DummyClassifier\n","\n","# Importing feature selection methods from additional libraries\n","from skrebate import ReliefF\n","from boruta import BorutaPy\n","\n","# Import SHAP for model explainability\n","import shap\n","\n","# Import oversampling technique from imbalanced-learn\n","from imblearn.over_sampling import ADASYN\n","\n","# Other libraries\n","import joblib\n","import statsmodels.api as sm\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","from scipy.stats import norm  # Statistical distributions\n","\n","# =============================================================================\n","# Global Settings and Parameters\n","# ==============================================================================\n","\n","# List of training dataset file paths (absolute paths)\n","TRAINING_FILES = [\n","    r\"Five fold cross Input\"  # <-- Example: Replace with your real file path\n","]\n","# âž¤ You can provide 1 to 3 training files\n","# âž¤ Supported formats: .csv, .xlsx, .xls\n","# Optional: Custom folder to store result Excel files\n","RESULTS_FOLDER = r\"Saving folder\"\n","# âž¤ Example: r\"D:\\MyResults\"\n","# âž¤ If None, results will be saved to a default 'res' folder\n","\n","# Absolute path to external test dataset (CSV or Excel)\n","EXTERNAL_TEST_FILE = r\"External input\"\n","\n","#EXTERNAL_TEST_FILE = None\n","\n","#EXTERNAL_TEST_FILE = r\"C:\\Users\\mahyar\\OneDrive\\Desktop\\idh\\final\\data\\external_test.csv\"\n","# âž¤ Must contain the same columns as training data\n","# âž¤ Also requires 'PatientID' and 'Outcome' columns\n","\n","\n","# Choose analysis mode: 'Supervised' or 'Semi-Supervised'\n","ANALYSIS_MODE = 'Supervised'\n","# âž¤ Options:\n","#     'Supervised'       = Use only rows with labeled Outcome\n","#     'Semi-Supervised'  = Fill missing Outcome values using predicted labels\n","\n","\n","# List of outcome classifiers to be used for outcome prediction\n","OUTCOME_CLASSIFIERS = [\"Logistic_Regression\"]\n","# âž¤ Available classifier names (strings from INVOLVED_CLASSIFIERS):\n","#     - \"Logistic_Regression\"\n","#     - \"Decision_Tree\"\n","#     - \"Random_Forest\"\n","#     - \"Support_Vector_Machine\"\n","#     - \"K_Nearest_Neighbors\"\n","#     - \"XGBoost\"\n","#     - \"LightGBM\"\n","#     - \"MLP_Classifier\"\n","#     - \"Gaussian_Naive_Bayes\"\n","#     - etc.\n","\n","# Choose test mode: 'External' or 'Internal'\n","TEST_MODE = 'External'  # âž¤ If 'Internal', TEST_SIZE will be used\n","\n","\n","# Use the provided training files directly\n","dataset_files = TRAINING_FILES\n","\n","\n","\n","RANDOM_SEED = 42\n","# âž¤ Used to ensure reproducibility of results\n","\n","\n","N_FOLDS = 5\n","# âž¤ Number of folds for Stratified K-Fold cross-validation\n","# âž¤ Common values: 3, 5, 10\n","\n","\n","TEST_SIZE = 0.2\n","# âž¤ Only used if splitting from training set (not relevant when external test is provided)\n","\n","\n","NOF = 10\n","# âž¤ Number Of Features (NOF) to select\n","# âž¤ Affects feature selection and extraction\n","\n","\n","SELECTED_CLASSES = [0, 1]\n","# âž¤ List of valid class labels to keep from the Outcome column\n","\n","\n","CLASS_MAPPING = {0: 0, 1: 1}\n","# âž¤ Maps each class name to a numeric value\n","# âž¤ Make sure it matches the SELECTED_CLASSES order\n","\n","# --------------------- #\n","# Class Selection Percentages\n","# --------------------- #\n","# Define the percentage of patients to select from each class before mapping\n","# Values should be between 0 and 1\n","CLASS_SELECTION_PERCENT = {\n","    0: 1.0,    # 100%\n","    1: 1.0      # 100%\n","}\n","\n","\n","SCALING_METHOD = 'MinMaxScaler'\n","# âž¤ Options:\n","#     'MinMaxScaler'\n","#     'StandardScaler'\n","#     'RobustScaler'\n","#     'Normalizer'\n","#     'MaxAbsScaler'\n","# âž¤ Set to any invalid value to skip scaling\n","\n","\n","IMPUTATION_STRATEGY = 'mean'\n","# âž¤ Options:\n","#     'mean'           = replace missing values with column mean\n","#     'median'         = replace with column median\n","#     'most_frequent'  = replace with most common value\n","\n","\n","# Set logging level for debugging or tracking info\n","logging.getLogger().setLevel(logging.INFO)\n","\n","\n","\n","# =============================================================================\n","# Validate Dataset Paths\n","# =============================================================================\n","\n","# Check if all training files exist\n","for file_path in TRAINING_FILES:\n","    if not os.path.isfile(file_path):\n","        raise FileNotFoundError(f\"Training file '{file_path}' not found.\")\n","\n","# Check if external test file exists\n","if TEST_MODE == 'External':\n","    if not os.path.isfile(EXTERNAL_TEST_FILE):\n","        raise FileNotFoundError(f\"External test file '{EXTERNAL_TEST_FILE}' not found.\")\n","\n","# Warn user if external file is provided but TEST_MODE is not 'External'\n","if TEST_MODE != 'External' and EXTERNAL_TEST_FILE and os.path.isfile(EXTERNAL_TEST_FILE):\n","    print(\"âš ï¸ WARNING: External test file is provided but TEST_MODE is set to 'Internal'. The file will be ignored.\")\n","\n","\n","\n","\n","# =============================================================================\n","# Helper Functions\n","# =============================================================================\n","\n","def load_dataframe(file_path):\n","    \"\"\"\n","    Load a dataset from a CSV or Excel file into a pandas DataFrame.\n","    Automatically detects format from file extension.\n","    \"\"\"\n","    if file_path.lower().endswith(\".csv\"):\n","        return pd.read_csv(file_path)\n","    elif file_path.lower().endswith((\".xlsx\", \".xls\")):\n","        return pd.read_excel(file_path, engine=\"openpyxl\")\n","    else:\n","        raise ValueError(f\"Unsupported file type: {file_path}\")\n","\n","\n","def show_class_distribution(before_df, after_df, outcome_col='Outcome'):\n","    print(\"\\nðŸ“Š Class Distribution BEFORE Filtering:\")\n","    before_counts = before_df[outcome_col].value_counts(dropna=False)\n","    before_percent = (before_counts / before_counts.sum()) * 100\n","    for cls in before_counts.index:\n","        print(f\" - {cls}: {before_counts[cls]} samples ({before_percent[cls]:.1f}%)\")\n","\n","    print(\"\\nðŸ“‰ Class Distribution AFTER Filtering:\")\n","    after_counts = after_df[outcome_col].value_counts(dropna=False)\n","    after_percent = (after_counts / after_counts.sum()) * 100\n","    for cls in after_counts.index:\n","        print(f\" - {cls}: {after_counts[cls]} samples ({after_percent[cls]:.1f}%)\")\n","\n","\n","\n","def append_df_to_excel(filename, df, sheet_name='Sheet1', startrow=None, **to_excel_kwargs):\n","    \"\"\"\n","    Append a DataFrame to an existing Excel file.\n","    Creates the file if it does not exist, or appends to a specific sheet.\n","    \"\"\"\n","    from openpyxl import load_workbook\n","\n","    if not os.path.isfile(filename):\n","        df.to_excel(filename, sheet_name=sheet_name, index=False, **to_excel_kwargs)\n","        return\n","\n","    try:\n","        writer = pd.ExcelWriter(filename, engine='openpyxl', mode='a', if_sheet_exists='overlay')\n","        workbook = writer.book\n","\n","        if startrow is None:\n","            if sheet_name in workbook.sheetnames:\n","                startrow = workbook[sheet_name].max_row\n","            else:\n","                startrow = 0\n","\n","        df.to_excel(writer, sheet_name=sheet_name, startrow=startrow,\n","                    index=False, header=(startrow == 0), **to_excel_kwargs)\n","        writer.close()\n","    except Exception as e:\n","        logging.error(f\"Error writing to Excel file {filename} in sheet {sheet_name}: {e}\")\n","\n","\n","def compute_auc_and_specificity(model, X, y):\n","    \"\"\"\n","    Compute AUC and Specificity metrics for the given model and input data.\n","    Uses predict_proba or decision_function to compute probabilities.\n","    \"\"\"\n","    try:\n","        # If the model supports predict_proba, use it to get probability for the positive class.\n","        if hasattr(model, \"predict_proba\"):\n","            probs = model.predict_proba(X)[:, 1]\n","        # If predict_proba is not available, try decision_function.\n","        elif hasattr(model, \"decision_function\"):\n","            probs = model.decision_function(X)\n","        else:\n","            probs = None\n","        # Compute AUC if probabilities are available.\n","        auc = roc_auc_score(y, probs) if probs is not None else np.nan\n","    except Exception as e:\n","        logging.error(f\"Error computing AUC: {e}\")\n","        auc = np.nan\n","\n","    try:\n","        # Obtain the predicted class labels.\n","        y_pred = model.predict(X)\n","        # Calculate the confusion matrix values: True Negative, False Positive, False Negative, True Positive.\n","        tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n","        # Calculate Specificity as TN/(TN+FP)\n","        specificity = tn / (tn + fp) if (tn + fp) != 0 else np.nan\n","    except Exception as e:\n","        logging.error(f\"Error computing Specificity: {e}\")\n","        specificity = np.nan\n","\n","    return auc, specificity\n","\n","def train_improved_autoencoder(X, encoding_dim, epochs=100, batch_size=32):\n","    \"\"\"\n","    Sample function to train an improved autoencoder.\n","    In this version, it is not implemented and simply returns None.\n","    \"\"\"\n","    logging.warning(\"train_improved_autoencoder is not implemented; returning None.\")\n","    return None\n","\n","def integrate_autoencoder_features(X, encoder, encoding_dim):\n","    \"\"\"\n","    Integrate features extracted by an autoencoder with the original data.\n","    In this version, it is not implemented and the original data is returned.\n","    \"\"\"\n","    logging.warning(\"integrate_autoencoder_features is not implemented; returning original data.\")\n","    return X\n","\n","# =============================================================================\n","# Feature Selection and Extraction Functions\n","# =============================================================================\n","\n","def apply_chi_square(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection using the Chi-Square test.\n","    Falls back to mutual_info_classif in case of an error.\n","    \"\"\"\n","    try:\n","        # Select K best features using the chi2 score function.\n","        selector = SelectKBest(score_func=chi2, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector\n","    except Exception as e:\n","        logging.error(f\"Error during Chi-Square feature selection: {e}\")\n","        # Fall back using mutual information if an error occurs\n","        selector = SelectKBest(score_func=mutual_info_classif, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector\n","\n","def apply_correlation_coefficient(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection based on correlation coefficients.\n","    Falls back to f_classif if an error occurs.\n","    \"\"\"\n","    try:\n","        # Compute the correlation matrix of the training data (features only).\n","        corr_matrix = np.corrcoef(X_train, rowvar=False)\n","        # Obtain correlations of each feature with the target (assumed to be in the last column).\n","        target_corr = corr_matrix[-1][:-1]\n","        # Select the indices of the top absolute correlations.\n","        top_idx = np.argsort(np.abs(target_corr))[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in correlation-based feature selection: {e}\")\n","        selector = SelectKBest(score_func=f_classif, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector.get_support(indices=True)\n","\n","def apply_mutual_information(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection using Mutual Information.\n","    Falls back to Chi2 in case of an error.\n","    \"\"\"\n","    try:\n","        selector = SelectKBest(score_func=mutual_info_classif, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector\n","    except Exception as e:\n","        logging.error(f\"Error during Mutual Information feature selection: {e}\")\n","        selector = SelectKBest(score_func=chi2, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector\n","\n","def apply_variance_threshold(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection using a variance threshold.\n","    Falls back to f_classif if any error occurs.\n","    \"\"\"\n","    try:\n","        selector = VarianceThreshold()\n","        X_var = selector.fit_transform(X_train)\n","        variances = selector.variances_\n","        # Select features with the highest variances.\n","        top_idx = np.argsort(variances)[-num_features:]\n","        return X_var[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in Variance Threshold feature selection: {e}\")\n","        selector = SelectKBest(score_func=f_classif, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector.get_support(indices=True)\n","\n","def apply_anova_f_test(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection using the ANOVA F-test.\n","    Falls back to using mutual information in case of error.\n","    \"\"\"\n","    try:\n","        selector = SelectKBest(score_func=f_classif, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector\n","    except Exception as e:\n","        logging.error(f\"Error in ANOVA F-test: {e}\")\n","        selector = SelectKBest(score_func=mutual_info_classif, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector\n","\n","def apply_information_gain(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection based on Information Gain (functionally similar to Mutual Information).\n","    \"\"\"\n","    try:\n","        selector = SelectKBest(score_func=mutual_info_classif, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector\n","    except Exception as e:\n","        logging.error(f\"Error in Information Gain feature selection: {e}\")\n","        selector = SelectKBest(score_func=chi2, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector\n","\n","def apply_univariate_feature_selection(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Univariate feature selection using a specified scoring function.\n","    Defaults to mutual information if no scoring function is provided.\n","    Falls back to f_classif in case of an error.\n","    \"\"\"\n","    try:\n","        score_func = kwargs.get('score_func', mutual_info_classif)\n","        selector = SelectKBest(score_func=score_func, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector\n","    except Exception as e:\n","        logging.error(f\"Error in univariate feature selection: {e}\")\n","        selector = SelectKBest(score_func=f_classif, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector\n","\n","def apply_lasso_fs(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection using LASSO regression.\n","    Falls back to Chi2 if an error occurs.\n","    \"\"\"\n","    try:\n","        alpha_ = kwargs.get('alpha', 0.01)\n","        lasso = Lasso(alpha=alpha_)\n","        lasso.fit(X_train, y_train)\n","        # Use the absolute coefficient values to rank features.\n","        coefs = np.abs(lasso.coef_)\n","        top_idx = np.argsort(coefs)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in LASSO feature selection: {e}\")\n","        selector = SelectKBest(score_func=chi2, k=num_features)\n","        X_sel = selector.fit_transform(X_train, y_train)\n","        return X_sel, selector.get_support(indices=True)\n","\n","def apply_rfe_fs(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection using Recursive Feature Elimination (RFE).\n","    Falls back to f_classif-based selection if an error occurs.\n","    \"\"\"\n","    try:\n","        estimator = LogisticRegression(max_iter=1000)\n","        rfe = RFE(estimator=estimator, n_features_to_select=num_features, step=1)\n","        rfe.fit(X_train, y_train)\n","        selected_idx = rfe.get_support(indices=True)\n","        return X_train[:, selected_idx], selected_idx\n","    except Exception as e:\n","        logging.error(f\"Error in RFE feature selection: {e}\")\n","        selector = SelectKBest(score_func=f_classif, k=num_features)\n","        X_sel = selector.fit_transform(X_train, y_train)\n","        return X_sel, selector.get_support(indices=True)\n","\n","def apply_elastic_net_fs(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection using the Elastic Net method.\n","    Falls back to LASSO if an error occurs.\n","    \"\"\"\n","    try:\n","        alpha_ = kwargs.get('alpha', 0.01)\n","        l1_ratio_ = kwargs.get('l1_ratio', 0.5)\n","        en = ElasticNet(alpha=alpha_, l1_ratio=l1_ratio_, random_state=42)\n","        en.fit(X_train, y_train)\n","        coefs = np.abs(en.coef_)\n","        top_idx = np.argsort(coefs)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in Elastic Net feature selection: {e}\")\n","        lasso = Lasso(alpha=0.01)\n","        lasso.fit(X_train, y_train)\n","        coefs = np.abs(lasso.coef_)\n","        top_idx = np.argsort(coefs)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","\n","def apply_sequential_feature_selector_func(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Sequential Feature Selector for stepwise feature selection.\n","    Falls back to RFE if any error occurs.\n","    \"\"\"\n","    try:\n","        direction = kwargs.get('direction', 'forward')\n","        scoring = kwargs.get('scoring', 'accuracy')\n","        cv = kwargs.get('cv', 3)\n","        estimator = LogisticRegression(max_iter=1000, random_state=42)\n","        selector = SequentialFeatureSelector(estimator=estimator, n_features_to_select=num_features,\n","                                             direction=direction, scoring=scoring, cv=cv, n_jobs=-1)\n","        selector.fit(X_train, y_train)\n","        selected_idx = selector.get_support(indices=True)\n","        return X_train[:, selected_idx], selected_idx\n","    except Exception as e:\n","        logging.error(f\"Error in sequential feature selection: {e}\")\n","        rfe = RFE(estimator=LogisticRegression(max_iter=1000, random_state=42),\n","                  n_features_to_select=num_features)\n","        rfe.fit(X_train, y_train)\n","        selected_idx = rfe.get_support(indices=True)\n","        return X_train[:, selected_idx], selected_idx\n","\n","def apply_select_fdr(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection with False Discovery Rate (FDR) control.\n","    Falls back to SelectFpr if an error occurs.\n","    \"\"\"\n","    try:\n","        score_func_ = kwargs.get('score_func', f_classif)\n","        alpha_ = kwargs.get('alpha', 0.05)\n","        selector = SelectFdr(score_func=score_func_, alpha=alpha_)\n","        selector.fit(X_train, y_train)\n","        scores = selector.scores_\n","        top_idx = np.argsort(scores)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in FDR feature selection: {e}\")\n","        from sklearn.feature_selection import SelectFpr\n","        selector = SelectFpr(score_func=f_classif, alpha=0.05)\n","        selector.fit(X_train, y_train)\n","        scores = selector.scores_\n","        top_idx = np.argsort(scores)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","\n","def apply_select_fwe(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection with Family-Wise Error (FWE) control.\n","    Falls back to SelectFpr if an error occurs.\n","    \"\"\"\n","    try:\n","        score_func_ = kwargs.get('score_func', f_classif)\n","        alpha_ = kwargs.get('alpha', 0.05)\n","        selector = SelectFwe(score_func=score_func_, alpha=alpha_)\n","        selector.fit(X_train, y_train)\n","        scores = selector.scores_\n","        top_idx = np.argsort(scores)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in FWE feature selection: {e}\")\n","        from sklearn.feature_selection import SelectFpr\n","        selector = SelectFpr(score_func=f_classif, alpha=0.05)\n","        selector.fit(X_train, y_train)\n","        scores = selector.scores_\n","        top_idx = np.argsort(scores)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","\n","def apply_feature_importance_rf(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection based on feature importances from a Random Forest model.\n","    Falls back to mutual_info_classif-based selection if an error occurs.\n","    \"\"\"\n","    try:\n","        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","        rf.fit(X_train, y_train)\n","        importances = rf.feature_importances_\n","        top_idx = np.argsort(importances)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in Random Forest importance feature selection: {e}\")\n","        selector = SelectKBest(score_func=mutual_info_classif, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector.get_support(indices=True)\n","\n","def apply_permutation_importance_fs(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection using permutation importance.\n","    Falls back to Random Forest based feature importance if an error occurs.\n","    \"\"\"\n","    from sklearn.inspection import permutation_importance\n","    try:\n","        scoring = kwargs.get('scoring', 'accuracy')\n","        model = LogisticRegression(max_iter=1000, random_state=42)\n","        model.fit(X_train, y_train)\n","        result = permutation_importance(model, X_train, y_train,\n","                                        n_repeats=10, random_state=42, n_jobs=-1, scoring=scoring)\n","        importances = result.importances_mean\n","        top_idx = np.argsort(importances)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in permutation importance feature selection: {e}\")\n","        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","        rf.fit(X_train, y_train)\n","        importances = rf.feature_importances_\n","        top_idx = np.argsort(importances)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","\n","def apply_relief_f(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection using the ReliefF algorithm.\n","    Falls back to f_classif based selection if an error occurs.\n","    \"\"\"\n","    try:\n","        n_feat = kwargs.get('n_features_to_select', 10)\n","        relief = ReliefF(n_features_to_select=n_feat)\n","        relief.fit(X_train, y_train)\n","        top_idx = relief.top_features_[:num_features]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in ReliefF feature selection: {e}\")\n","        selector = SelectKBest(score_func=f_classif, k=num_features)\n","        X_selected = selector.fit_transform(X_train, y_train)\n","        return X_selected, selector.get_support(indices=True)\n","\n","def apply_mutual_info_selection(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection using mutual information.\n","    Falls back to f_classif in case an error occurs.\n","    \"\"\"\n","    try:\n","        selector = SelectKBest(score_func=mutual_info_classif, k=num_features)\n","        X_sel = selector.fit_transform(X_train, y_train)\n","        return X_sel, selector\n","    except Exception as e:\n","        logging.error(f\"Error in mutual information feature selection: {e}\")\n","        selector = SelectKBest(score_func=f_classif, k=num_features)\n","        X_sel = selector.fit_transform(X_train, y_train)\n","        return X_sel, selector\n","\n","def apply_select_from_model_lr(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection using Logistic Regression as a SelectFromModel method.\n","    Falls back to RFE if an error occurs.\n","    \"\"\"\n","    from sklearn.feature_selection import SelectFromModel\n","    try:\n","        lr = LogisticRegression(max_iter=1000, random_state=42)\n","        selector = SelectFromModel(estimator=lr, max_features=num_features, prefit=False)\n","        selector.fit(X_train, y_train)\n","        selected_idx = selector.get_support(indices=True)\n","        return X_train[:, selected_idx], selected_idx\n","    except Exception as e:\n","        logging.error(f\"Error in model-based (LR) feature selection: {e}\")\n","        rfe = RFE(estimator=LogisticRegression(max_iter=1000, random_state=42), n_features_to_select=num_features)\n","        rfe.fit(X_train, y_train)\n","        selected_idx = rfe.get_support(indices=True)\n","        return X_train[:, selected_idx], selected_idx\n","\n","def apply_extra_trees_importance(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection based on feature importances from an Extra Trees model.\n","    Falls back to mutual_info_classif based selection if an error occurs.\n","    \"\"\"\n","    try:\n","        et = ExtraTreesClassifier(n_estimators=100, random_state=42)\n","        et.fit(X_train, y_train)\n","        importances = et.feature_importances_\n","        top_idx = np.argsort(importances)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in Extra Trees feature importance selection: {e}\")\n","        selector = SelectKBest(score_func=mutual_info_classif, k=num_features)\n","        X_sel = selector.fit_transform(X_train, y_train)\n","        return X_sel, selector.get_support(indices=True)\n","\n","def apply_embedded_elastic_net(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Embedded feature selection using Elastic Net.\n","    Falls back to LASSO if an error occurs.\n","    \"\"\"\n","    try:\n","        en = ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42)\n","        en.fit(X_train, y_train)\n","        coefs = np.abs(en.coef_)\n","        top_idx = np.argsort(coefs)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in embedded Elastic Net feature selection: {e}\")\n","        lasso = Lasso(alpha=0.01)\n","        lasso.fit(X_train, y_train)\n","        coefs = np.abs(lasso.coef_)\n","        top_idx = np.argsort(coefs)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","\n","def apply_pca_loadings_selection(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection based on PCA loadings.\n","    Sums the absolute values of the PCA components and selects the top features.\n","    Falls back to chi2-based selection if any error occurs.\n","    \"\"\"\n","    try:\n","        pca = PCA(n_components=num_features)\n","        pca.fit(X_train)\n","        loadings = np.abs(pca.components_).sum(axis=0)\n","        top_idx = np.argsort(loadings)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in PCA loadings feature selection: {e}\")\n","        selector = SelectKBest(score_func=chi2, k=num_features)\n","        X_sel = selector.fit_transform(X_train, y_train)\n","        return X_sel, selector.get_support(indices=True)\n","\n","def apply_pca_dictionary(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using PCA and return the transformed data as a 'dictionary' (placeholder).\n","    Falls back to TruncatedSVD if PCA fails.\n","    \"\"\"\n","    try:\n","        pca = PCA(n_components=num_features)\n","        X_pca = pca.fit_transform(X_train)\n","        return X_pca, pca\n","    except Exception as e:\n","        logging.error(f\"Error in PCA dictionary feature extraction: {e}\")\n","        svd = TruncatedSVD(n_components=num_features, random_state=42)\n","        X_svd = svd.fit_transform(X_train)\n","        return X_svd, svd\n","\n","def apply_vif_selection(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection based on Variance Inflation Factor (VIF).\n","    If an error occurs, falls back to f_classif based feature selection.\n","    \"\"\"\n","    try:\n","        # Add a constant column for VIF calculation\n","        X_const = sm.add_constant(X_train)\n","        # Compute VIF for each feature, ignoring the constant (index 0)\n","        vif_scores = [variance_inflation_factor(X_const, i) for i in range(1, X_const.shape[1])]\n","        vif_array = np.array(vif_scores)\n","        # Select features with the smallest VIF (lowest multicollinearity)\n","        top_idx = np.argsort(vif_array)[:num_features]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in VIF feature selection: {e}\")\n","        selector = SelectKBest(score_func=f_classif, k=num_features)\n","        X_sel = selector.fit_transform(X_train, y_train)\n","        return X_sel, selector.get_support(indices=True)\n","\n","def apply_stability_lasso_selection(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Stable feature selection using repeated LASSO runs.\n","    Averages the absolute coefficients over multiple runs and selects features with the highest average.\n","    Falls back to mutual_info_classif based selection on error.\n","    \"\"\"\n","    try:\n","        n_runs = kwargs.get('n_runs', 10)\n","        alphas = kwargs.get('alphas', [0.01])\n","        coef_sum = np.zeros(X_train.shape[1])\n","        for i in range(n_runs):\n","            # Add a small noise to the data to stabilize the selection\n","            X_noisy = X_train + np.random.normal(0, 0.001, X_train.shape)\n","            model = Lasso(alpha=np.random.choice(alphas), random_state=42 + i)\n","            model.fit(X_noisy, y_train)\n","            coef_sum += np.abs(model.coef_)\n","        avg_coefs = coef_sum / n_runs\n","        top_idx = np.argsort(avg_coefs)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in stable LASSO feature selection: {e}\")\n","        selector = SelectKBest(score_func=mutual_info_classif, k=num_features)\n","        X_sel = selector.fit_transform(X_train, y_train)\n","        return X_sel, selector.get_support(indices=True)\n","\n","def apply_mutual_info_gain_ratio(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection using the ratio of mutual information to standard deviation.\n","    Falls back to mutual_info_classif based selection if an error occurs.\n","    \"\"\"\n","    try:\n","        mi = mutual_info_classif(X_train, y_train)\n","        stds = np.std(X_train, axis=0)\n","        stds[stds == 0] = 1e-6  # Avoid division by zero\n","        gain_ratio = mi / stds\n","        top_idx = np.argsort(gain_ratio)[-num_features:]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in mutual information gain ratio feature selection: {e}\")\n","        selector = SelectKBest(score_func=mutual_info_classif, k=num_features)\n","        X_sel = selector.fit_transform(X_train, y_train)\n","        return X_sel, selector\n","\n","def apply_chi2_pvalue_selection(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection based on the p-value from the Chi2 test.\n","    Falls back to chi2 based feature selection if an error occurs.\n","    \"\"\"\n","    try:\n","        scores, pvalues = chi2(X_train, y_train)\n","        top_idx = np.argsort(pvalues)[:num_features]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in Chi2 p-value feature selection: {e}\")\n","        selector = SelectKBest(score_func=chi2, k=num_features)\n","        X_sel = selector.fit_transform(X_train, y_train)\n","        return X_sel, selector\n","\n","def apply_anova_pvalue_selection(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Feature selection based on p-values from the ANOVA test.\n","    Falls back to f_classif based selection if an error occurs.\n","    \"\"\"\n","    try:\n","        scores, pvalues = f_classif(X_train, y_train)\n","        top_idx = np.argsort(pvalues)[:num_features]\n","        return X_train[:, top_idx], top_idx\n","    except Exception as e:\n","        logging.error(f\"Error in ANOVA p-value feature selection: {e}\")\n","        selector = SelectKBest(score_func=f_classif, k=num_features)\n","        X_sel = selector.fit_transform(X_train, y_train)\n","        return X_sel, selector\n","\n","# ------------------- Feature Extraction Methods -------------------\n","def apply_pca(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Principal Component Analysis (PCA).\n","    Falls back to TruncatedSVD if PCA fails.\n","    \"\"\"\n","    try:\n","        pca = PCA(n_components=num_features)\n","        X_pca = pca.fit_transform(X_train)\n","        return X_pca, pca\n","    except Exception as e:\n","        logging.error(f\"Error in PCA feature extraction: {e}\")\n","        svd = TruncatedSVD(n_components=num_features, random_state=42)\n","        X_svd = svd.fit_transform(X_train)\n","        return X_svd, svd\n","\n","def apply_ica(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Independent Component Analysis (ICA).\n","    Falls back to PCA if ICA fails.\n","    \"\"\"\n","    try:\n","        ica = FastICA(n_components=num_features, random_state=42)\n","        X_ica = ica.fit_transform(X_train)\n","        return X_ica, ica\n","    except Exception as e:\n","        logging.error(f\"Error in ICA feature extraction: {e}\")\n","        pca = PCA(n_components=num_features)\n","        X_pca = pca.fit_transform(X_train)\n","        return X_pca, pca\n","\n","def apply_t_sne(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using t-Distributed Stochastic Neighbor Embedding (t-SNE).\n","    Falls back to UMAP if t-SNE fails.\n","    \"\"\"\n","    try:\n","        tsne = TSNE(n_components=num_features, random_state=42)\n","        X_tsne = tsne.fit_transform(X_train)\n","        return X_tsne, tsne\n","    except Exception as e:\n","        logging.error(f\"Error in t-SNE feature extraction: {e}\")\n","        from umap import UMAP\n","        um = UMAP(n_components=num_features, random_state=42)\n","        X_umap = um.fit_transform(X_train)\n","        return X_umap, um\n","\n","def apply_umap_extraction(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using UMAP.\n","    Falls back to PCA if UMAP fails.\n","    \"\"\"\n","    try:\n","        from umap import UMAP\n","        um = UMAP(n_components=num_features, random_state=42)\n","        X_umap = um.fit_transform(X_train)\n","        return X_umap, um\n","    except Exception as e:\n","        logging.error(f\"Error in UMAP feature extraction: {e}\")\n","        pca = PCA(n_components=num_features)\n","        X_pca = pca.fit_transform(X_train)\n","        return X_pca, pca\n","\n","def apply_kernel_pca_extraction(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Kernel PCA with the RBF kernel.\n","    Falls back to PCA if an error occurs.\n","    \"\"\"\n","    try:\n","        kpca = KernelPCA(n_components=num_features, kernel='rbf', random_state=42)\n","        X_kpca = kpca.fit_transform(X_train)\n","        return X_kpca, kpca\n","    except Exception as e:\n","        logging.error(f\"Error in Kernel PCA feature extraction: {e}\")\n","        pca = PCA(n_components=num_features)\n","        X_pca = pca.fit_transform(X_train)\n","        return X_pca, pca\n","\n","def apply_truncated_svd_extraction(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Truncated SVD.\n","    Falls back to PCA if an error occurs.\n","    \"\"\"\n","    try:\n","        svd = TruncatedSVD(n_components=num_features, random_state=42)\n","        X_svd = svd.fit_transform(X_train)\n","        return X_svd, svd\n","    except Exception as e:\n","        logging.error(f\"Error in Truncated SVD feature extraction: {e}\")\n","        pca = PCA(n_components=num_features)\n","        X_pca = pca.fit_transform(X_train)\n","        return X_pca, pca\n","\n","def apply_feature_agglomeration_extraction(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Feature Agglomeration.\n","    Falls back to PCA if it fails.\n","    \"\"\"\n","    try:\n","        agg = FeatureAgglomeration(n_clusters=num_features)\n","        X_agg = agg.fit_transform(X_train)\n","        return X_agg, agg\n","    except Exception as e:\n","        logging.error(f\"Error in Feature Agglomeration extraction: {e}\")\n","        pca = PCA(n_components=num_features)\n","        X_pca = pca.fit_transform(X_train)\n","        return X_pca, pca\n","\n","def apply_isomap_extraction(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Isomap.\n","    Falls back to PCA if it fails.\n","    \"\"\"\n","    try:\n","        iso = Isomap(n_components=num_features)\n","        X_iso = iso.fit_transform(X_train)\n","        return X_iso, iso\n","    except Exception as e:\n","        logging.error(f\"Error in Isomap feature extraction: {e}\")\n","        pca = PCA(n_components=num_features)\n","        X_pca = pca.fit_transform(X_train)\n","        return X_pca, pca\n","\n","def apply_gaussian_random_projection_extraction(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Gaussian Random Projection.\n","    Falls back to Sparse Random Projection if an error occurs.\n","    \"\"\"\n","    from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n","    try:\n","        grp = GaussianRandomProjection(n_components=num_features, random_state=42)\n","        X_grp = grp.fit_transform(X_train)\n","        return X_grp, grp\n","    except Exception as e:\n","        logging.error(f\"Error in Gaussian Random Projection feature extraction: {e}\")\n","        srp = SparseRandomProjection(n_components=num_features, random_state=42)\n","        X_srp = srp.fit_transform(X_train)\n","        return X_srp, srp\n","\n","def apply_autoencoder_extraction(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using an Autoencoder (placeholder implementation).\n","    Falls back to PCA if autoencoder extraction is not implemented.\n","    \"\"\"\n","    try:\n","        encoder = kwargs.get('encoder')\n","        if encoder is None:\n","            encoder = train_improved_autoencoder(X_train, encoding_dim=num_features, epochs=100, batch_size=32)\n","        X_auto = integrate_autoencoder_features(X_train, encoder, encoding_dim=num_features)\n","        return X_auto, encoder\n","    except Exception as e:\n","        logging.error(f\"Error in Autoencoder feature extraction: {e}\")\n","        pca = PCA(n_components=num_features)\n","        X_pca = pca.fit_transform(X_train)\n","        return X_pca, pca\n","\n","def apply_truncated_pca(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Truncated PCA.\n","    Falls back to standard PCA if an error occurs.\n","    \"\"\"\n","    try:\n","        svd_ = TruncatedSVD(n_components=num_features, random_state=42)\n","        X_svd = svd_.fit_transform(X_train)\n","        return X_svd, svd_\n","    except Exception as e:\n","        logging.error(f\"Error in Truncated PCA feature extraction: {e}\")\n","        pca_ = PCA(n_components=num_features)\n","        X_pca_ = pca_.fit_transform(X_train)\n","        return X_pca_, pca_\n","\n","def apply_factor_analysis(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Factor Analysis.\n","    Falls back to PCA if an error occurs.\n","    \"\"\"\n","    try:\n","        n_components_ = kwargs.get('n_components', 10)\n","        fa = FactorAnalysis(n_components=n_components_, random_state=42)\n","        X_fa = fa.fit_transform(X_train)\n","        return X_fa, fa\n","    except Exception as e:\n","        logging.error(f\"Error in Factor Analysis feature extraction: {e}\")\n","        pca_ = PCA(n_components=num_features)\n","        X_pca_ = pca_.fit_transform(X_train)\n","        return X_pca_, pca_\n","\n","def apply_sparse_pca(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Sparse PCA.\n","    Falls back to FastICA if Sparse PCA fails.\n","    \"\"\"\n","    from sklearn.decomposition import SparsePCA, FastICA\n","    try:\n","        n_components_ = kwargs.get('n_components', 10)\n","        alpha_ = kwargs.get('alpha', 1)\n","        spca = SparsePCA(n_components=n_components_, alpha=alpha_, random_state=42)\n","        X_spca = spca.fit_transform(X_train)\n","        return X_spca, spca\n","    except Exception as e:\n","        logging.error(f\"Error in Sparse PCA feature extraction: {e}\")\n","        ica = FastICA(n_components=num_features, random_state=42)\n","        X_ica = ica.fit_transform(X_train)\n","        return X_ica, ica\n","\n","def apply_nmf(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Non-negative Matrix Factorization (NMF).\n","    Falls back to PCA if an error occurs.\n","    \"\"\"\n","    from sklearn.decomposition import NMF\n","    try:\n","        n_components_ = kwargs.get('n_components', 10)\n","        init_ = kwargs.get('init', 'random')\n","        random_state_ = kwargs.get('random_state', 42)\n","        max_iter_ = kwargs.get('max_iter', 200)\n","        nmf = NMF(n_components=n_components_, init=init_, random_state=random_state_, max_iter=max_iter_)\n","        X_nmf = nmf.fit_transform(X_train)\n","        return X_nmf, nmf\n","    except Exception as e:\n","        logging.error(f\"Error in NMF feature extraction: {e}\")\n","        pca_ = PCA(n_components=num_features)\n","        X_pca_ = pca_.fit_transform(X_train)\n","        return X_pca_, pca_\n","\n","def apply_fastica(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using FastICA.\n","    Falls back to PCA if an error occurs.\n","    \"\"\"\n","    try:\n","        n_components_ = kwargs.get('n_components', 10)\n","        random_state_ = kwargs.get('random_state', 42)\n","        ica = FastICA(n_components=n_components_, random_state=random_state_)\n","        X_ica = ica.fit_transform(X_train)\n","        return X_ica, ica\n","    except Exception as e:\n","        logging.error(f\"Error in FastICA feature extraction: {e}\")\n","        pca_ = PCA(n_components=num_features)\n","        X_pca_ = pca_.fit_transform(X_train)\n","        return X_pca_, pca_\n","\n","def apply_independent_component_analysis(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Independent Component Analysis (ICA).\n","    Falls back to PCA if ICA fails.\n","    \"\"\"\n","    try:\n","        ica = FastICA(n_components=num_features, random_state=42)\n","        X_ica = ica.fit_transform(X_train)\n","        return X_ica, ica\n","    except Exception as e:\n","        logging.error(f\"Error in Independent Component Analysis extraction: {e}\")\n","        pca_ = PCA(n_components=num_features)\n","        X_pca_ = pca_.fit_transform(X_train)\n","        return X_pca_, pca_\n","\n","def apply_locally_linear_embedding(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Locally Linear Embedding (LLE).\n","    Falls back to MDS if an error occurs.\n","    \"\"\"\n","    try:\n","        n_components_ = kwargs.get('n_components', 10)\n","        lle = LocallyLinearEmbedding(n_components=n_components_, random_state=42)\n","        X_lle = lle.fit_transform(X_train)\n","        return X_lle, lle\n","    except Exception as e:\n","        logging.error(f\"Error in Locally Linear Embedding extraction: {e}\")\n","        mds_ = MDS(n_components=num_features, random_state=42)\n","        X_mds = mds_.fit_transform(X_train)\n","        return X_mds, mds_\n","\n","def apply_mds(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Multidimensional Scaling (MDS).\n","    Falls back to Spectral Embedding if an error occurs.\n","    \"\"\"\n","    try:\n","        n_components_ = kwargs.get('n_components', 10)\n","        random_state_ = kwargs.get('random_state', 42)\n","        mds_ = MDS(n_components=n_components_, random_state=random_state_)\n","        X_mds = mds_.fit_transform(X_train)\n","        return X_mds, mds_\n","    except Exception as e:\n","        logging.error(f\"Error in MDS feature extraction: {e}\")\n","        se_ = SpectralEmbedding(n_components=num_features, random_state=42)\n","        X_se = se_.fit_transform(X_train)\n","        return X_se, se_\n","\n","def apply_isomap_alternative(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Alternative feature extraction using Isomap.\n","    Falls back to PCA if an error occurs.\n","    \"\"\"\n","    try:\n","        iso = Isomap(n_components=num_features)\n","        X_iso = iso.fit_transform(X_train)\n","        return X_iso, iso\n","    except Exception as e:\n","        logging.error(f\"Error in alternative Isomap feature extraction: {e}\")\n","        pca_ = PCA(n_components=num_features)\n","        X_pca_ = pca_.fit_transform(X_train)\n","        return X_pca_, pca_\n","\n","def apply_autoencoder_deep(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using a deep autoencoder (placeholder).\n","    If the autoencoder extraction fails, the original data is returned.\n","    \"\"\"\n","    try:\n","        encoder = kwargs.get('encoder')\n","        if encoder is None:\n","            encoder = train_improved_autoencoder(X_train, encoding_dim=num_features, epochs=100, batch_size=32)\n","        X_auto = integrate_autoencoder_features(X_train, encoder, encoding_dim=num_features)\n","        return X_auto, encoder\n","    except Exception as e:\n","        logging.error(f\"Error in deep autoencoder feature extraction: {e}\")\n","        encoder = None\n","        return X_train, encoder\n","\n","def apply_sparse_random_projection_extraction(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Sparse Random Projection.\n","    Falls back to Gaussian Random Projection if an error occurs.\n","    \"\"\"\n","    from sklearn.random_projection import SparseRandomProjection, GaussianRandomProjection\n","    try:\n","        srp = SparseRandomProjection(n_components=num_features, random_state=42)\n","        X_srp = srp.fit_transform(X_train)\n","        return X_srp, srp\n","    except Exception as e:\n","        logging.error(f\"Error in Sparse Random Projection extraction: {e}\")\n","        grp = GaussianRandomProjection(n_components=num_features, random_state=42)\n","        X_grp = grp.fit_transform(X_train)\n","        return X_grp, grp\n","\n","def apply_lda_extraction(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Linear Discriminant Analysis (LDA).\n","    Falls back to PCA if LDA fails.\n","    \"\"\"\n","    try:\n","        lda = LinearDiscriminantAnalysis(n_components=num_features)\n","        X_lda = lda.fit_transform(X_train, y_train)\n","        return X_lda, lda\n","    except Exception as e:\n","        logging.error(f\"Error in LDA feature extraction: {e}\")\n","        pca_ = PCA(n_components=num_features)\n","        X_pca_ = pca_.fit_transform(X_train)\n","        return X_pca_, pca_\n","\n","def apply_spectral_embedding_extraction(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Spectral Embedding.\n","    Falls back to PCA if an error occurs.\n","    \"\"\"\n","    try:\n","        se_ = SpectralEmbedding(n_components=num_features, random_state=42)\n","        X_se = se_.fit_transform(X_train)\n","        return X_se, se_\n","    except Exception as e:\n","        logging.error(f\"Error in Spectral Embedding extraction: {e}\")\n","        pca_ = PCA(n_components=num_features)\n","        X_pca_ = pca_.fit_transform(X_train)\n","        return X_pca_, pca_\n","\n","def apply_mds_different_params(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using MDS with different parameters (a variation).\n","    Falls back to UMAP if an error occurs.\n","    \"\"\"\n","    try:\n","        mds_ = MDS(n_components=num_features, random_state=42)\n","        X_mds = mds_.fit_transform(X_train)\n","        return X_mds, mds_\n","    except Exception as e:\n","        logging.error(f\"Error in MDS with different parameters: {e}\")\n","        from umap import UMAP\n","        um_ = UMAP(n_components=num_features, random_state=42)\n","        X_umap = um_.fit_transform(X_train)\n","        return X_umap, um_\n","\n","def apply_t_sne_enhanced(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using an enhanced version of t-SNE.\n","    Falls back to UMAP if enhanced t-SNE fails.\n","    \"\"\"\n","    try:\n","        tsne = TSNE(n_components=num_features, random_state=42)\n","        X_tsne = tsne.fit_transform(X_train)\n","        return X_tsne, tsne\n","    except Exception as e:\n","        logging.error(f\"Error in enhanced t-SNE feature extraction: {e}\")\n","        from umap import UMAP\n","        umap_ = UMAP(n_components=num_features, random_state=42)\n","        X_umap = umap_.fit_transform(X_train)\n","        return X_umap, umap_\n","\n","def apply_feature_hashing(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Feature Hashing.\n","    Falls back to PCA if an error occurs.\n","    \"\"\"\n","    from sklearn.feature_extraction import FeatureHasher\n","    try:\n","        hasher = FeatureHasher(n_features=num_features, input_type=\"matrix\")\n","        X_hashed = hasher.transform(X_train)\n","        X_hashed = X_hashed.toarray()  # Convert sparse matrix to dense array\n","        return X_hashed, hasher\n","    except Exception as e:\n","        logging.error(f\"Error in Feature Hashing extraction: {e}\")\n","        pca_ = PCA(n_components=num_features)\n","        X_pca_ = pca_.fit_transform(X_train)\n","        return X_pca_, pca_\n","\n","def apply_umap_min_dist_extraction(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using UMAP with a specified minimum distance (min_dist).\n","    Falls back to PCA if an error occurs.\n","    \"\"\"\n","    try:\n","        from umap import UMAP\n","        min_dist_ = kwargs.get('min_dist', 0.1)\n","        umap_ = UMAP(n_components=num_features, random_state=42, min_dist=min_dist_)\n","        X_umap = umap_.fit_transform(X_train)\n","        return X_umap, umap_\n","    except Exception as e:\n","        logging.error(f\"Error in UMAP (min_dist) feature extraction: {e}\")\n","        pca_ = PCA(n_components=num_features)\n","        X_pca_ = pca_.fit_transform(X_train)\n","        return X_pca_, pca_\n","\n","def apply_kernel_pca_poly_extraction(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Kernel PCA with a polynomial kernel.\n","    Falls back to PCA if an error occurs.\n","    \"\"\"\n","    try:\n","        degree_ = kwargs.get('degree', 3)\n","        kpca_ = KernelPCA(n_components=num_features, kernel='poly', degree=degree_, random_state=42)\n","        X_kpca = kpca_.fit_transform(X_train)\n","        return X_kpca, kpca_\n","    except Exception as e:\n","        logging.error(f\"Error in Kernel PCA (poly) feature extraction: {e}\")\n","        pca_ = PCA(n_components=num_features)\n","        X_pca_ = pca_.fit_transform(X_train)\n","        return X_pca_, pca_\n","\n","def apply_diffusion_map_extraction(X_train, y_train, num_features, **kwargs):\n","    \"\"\"\n","    Extract features using Diffusion Map via Spectral Embedding with nearest neighbor affinity.\n","    Falls back to PCA if an error occurs.\n","    \"\"\"\n","    from sklearn.manifold import SpectralEmbedding\n","    try:\n","        diffusion = SpectralEmbedding(n_components=num_features, affinity='nearest_neighbors', random_state=42)\n","        X_diff = diffusion.fit_transform(X_train)\n","        return X_diff, diffusion\n","    except Exception as e:\n","        logging.error(f\"Error in Diffusion Map feature extraction: {e}\")\n","        pca_ = PCA(n_components=num_features)\n","        X_pca_ = pca_.fit_transform(X_train)\n","        return X_pca_, pca_\n","\n","# =============================================================================\n","# Mapping dictionaries for Feature Selection Functions and Classifiers\n","# =============================================================================\n","\n","# Dictionary mapping names to corresponding feature selection function objects.\n","feature_selector_functions = {\n","    \"apply_chi_square\": apply_chi_square,\n","    \"apply_correlation_coefficient\": apply_correlation_coefficient,\n","    \"apply_mutual_information\": apply_mutual_information,\n","    \"apply_variance_threshold\": apply_variance_threshold,\n","    \"apply_anova_f_test\": apply_anova_f_test,\n","    \"apply_information_gain\": apply_information_gain,\n","    \"apply_univariate_feature_selection\": apply_univariate_feature_selection,\n","    \"apply_lasso_fs\": apply_lasso_fs,\n","    \"apply_rfe_fs\": apply_rfe_fs,\n","    \"apply_elastic_net_fs\": apply_elastic_net_fs,\n","    \"apply_sequential_feature_selector_func\": apply_sequential_feature_selector_func,\n","    \"apply_select_fdr\": apply_select_fdr,\n","    \"apply_select_fwe\": apply_select_fwe,\n","    \"apply_feature_importance_rf\": apply_feature_importance_rf,\n","    \"apply_permutation_importance_fs\": apply_permutation_importance_fs,\n","    \"apply_relief_f\": apply_relief_f,\n","    \"apply_mutual_info_selection\": apply_mutual_info_selection,\n","    \"apply_select_from_model_lr\": apply_select_from_model_lr,\n","    \"apply_extra_trees_importance\": apply_extra_trees_importance,\n","    \"apply_embedded_elastic_net\": apply_embedded_elastic_net,\n","    \"apply_pca_loadings_selection\": apply_pca_loadings_selection,\n","    \"apply_pca_dictionary\": apply_pca_dictionary,\n","    \"apply_vif_selection\": apply_vif_selection,\n","    \"apply_stability_lasso_selection\": apply_stability_lasso_selection,\n","    \"apply_mutual_info_gain_ratio\": apply_mutual_info_gain_ratio,\n","    \"apply_chi2_pvalue_selection\": apply_chi2_pvalue_selection,\n","    \"apply_anova_pvalue_selection\": apply_anova_pvalue_selection,\n","    # Feature extraction functions:\n","    \"apply_pca\": apply_pca,\n","    \"apply_ica\": apply_ica,\n","    \"apply_t_sne\": apply_t_sne,\n","    \"apply_umap_extraction\": apply_umap_extraction,\n","    \"apply_kernel_pca_extraction\": apply_kernel_pca_extraction,\n","    \"apply_truncated_svd_extraction\": apply_truncated_svd_extraction,\n","    \"apply_feature_agglomeration_extraction\": apply_feature_agglomeration_extraction,\n","    \"apply_isomap_extraction\": apply_isomap_extraction,\n","    \"apply_gaussian_random_projection_extraction\": apply_gaussian_random_projection_extraction,\n","    \"apply_autoencoder_extraction\": apply_autoencoder_extraction,\n","    \"apply_truncated_pca\": apply_truncated_pca,\n","    \"apply_factor_analysis\": apply_factor_analysis,\n","    \"apply_sparse_pca\": apply_sparse_pca,\n","    \"apply_nmf\": apply_nmf,\n","    \"apply_fastica\": apply_fastica,\n","    \"apply_independent_component_analysis\": apply_independent_component_analysis,\n","    \"apply_locally_linear_embedding\": apply_locally_linear_embedding,\n","    \"apply_mds\": apply_mds,\n","    \"apply_isomap_alternative\": apply_isomap_alternative,\n","    \"apply_autoencoder_deep\": apply_autoencoder_deep,\n","    \"apply_sparse_random_projection_extraction\": apply_sparse_random_projection_extraction,\n","    \"apply_lda_extraction\": apply_lda_extraction,\n","    \"apply_spectral_embedding_extraction\": apply_spectral_embedding_extraction,\n","    \"apply_mds_different_params\": apply_mds_different_params,\n","    \"apply_t_sne_enhanced\": apply_t_sne_enhanced,\n","    \"apply_feature_hashing\": apply_feature_hashing,\n","    \"apply_umap_min_dist_extraction\": apply_umap_min_dist_extraction,\n","    \"apply_kernel_pca_poly_extraction\": apply_kernel_pca_poly_extraction,\n","    \"apply_diffusion_map_extraction\": apply_diffusion_map_extraction,\n","}\n","\n","# =============================================================================\n","# Classifier Dictionaries\n","# =============================================================================\n","INVOLVED_CLASSIFIERS = {\n","    'Decision_Tree': {\n","        'model': DecisionTreeClassifier,\n","        'params': {'random_state': 42, 'class_weight': 'balanced'},\n","        'param_grid': {'max_depth': [None, 10, 20, 30],\n","                       'min_samples_split': [2, 5, 10],\n","                       'min_samples_leaf': [1, 2, 4],\n","                       'criterion': ['gini', 'entropy']}\n","    },\n","    'Logistic_Regression': {\n","        'model': LogisticRegression,\n","        'params': {'max_iter': 1000, 'random_state': 42, 'class_weight': 'balanced'},\n","        'param_grid': {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","                       'penalty': ['l2'],\n","                       'solver': ['lbfgs', 'saga']}\n","    },\n","    'Linear_Discriminant_Analysis': {\n","        'model': LinearDiscriminantAnalysis,\n","        'params': {},\n","        'param_grid': [{'solver': ['svd']},\n","                       {'solver': ['lsqr', 'eigen'], 'shrinkage': [None, 'auto']}]\n","    },\n","    'Gaussian_Naive_Bayes': {\n","        'model': GaussianNB,\n","        'params': {},\n","        'param_grid': {'var_smoothing': [1e-09, 1e-08, 1e-07]}\n","    },\n","    'Bernoulli_Naive_Bayes': {\n","        'model': BernoulliNB,\n","        'params': {'alpha': 1.0, 'binarize': 0.0},\n","        'param_grid': {'alpha': [0.1, 1.0, 10.0],\n","                       'binarize': [0.0, 0.5, 1.0]}\n","    },\n","    'Complement_Naive_Bayes': {\n","        'model': ComplementNB,\n","        'params': {'alpha': 1.0},\n","        'param_grid': {'alpha': [0.1, 1.0, 10.0]}\n","    },\n","    'K_Nearest_Neighbors': {\n","        'model': KNeighborsClassifier,\n","        'params': {'n_neighbors': 5},\n","        'param_grid': {'n_neighbors': [3, 5, 7, 9],\n","                       'weights': ['uniform', 'distance'],\n","                       'metric': ['euclidean', 'manhattan', 'minkowski']}\n","    },\n","    'Random_Forest': {\n","        'model': RandomForestClassifier,\n","        'params': {'random_state': 42, 'class_weight': 'balanced'},\n","        'param_grid': {'n_estimators': [50, 100, 200, 400],\n","                       'max_depth': [None, 10, 20, 30, 50],\n","                       'min_samples_split': [2, 5, 10, 15],\n","                       'min_samples_leaf': [1, 2, 4, 8]}\n","    },\n","    'Ridge_Classifier': {\n","        'model': RidgeClassifier,\n","        'params': {'alpha': 1.0, 'random_state': 42},\n","        'param_grid': {'alpha': [0.1, 1.0, 10.0, 100.0]}\n","    },\n","    'Support_Vector_Machine': {\n","        'model': SVC,\n","        'params': {'random_state': 42, 'class_weight': 'balanced'},\n","        'param_grid': {'C': [0.01, 0.1, 1, 10],\n","                       'kernel': ['linear', 'rbf', 'poly'],\n","                       'gamma': ['scale', 'auto']}\n","    },\n","    'XGBoost': {\n","        'model': XGBClassifier,\n","        'params': {'random_state': 42, 'use_label_encoder': False, 'eval_metric': 'logloss'},\n","        'param_grid': {'n_estimators': [50, 100, 200, 300],\n","                       'learning_rate': [0.001, 0.01, 0.1, 0.2],\n","                       'max_depth': [3, 5, 7, 9]}\n","    },\n","    'LightGBM': {\n","        'model': LGBMClassifier,\n","        'params': {'random_state': 42},\n","        'param_grid': {'n_estimators': [50, 100, 200],\n","                       'learning_rate': [0.01, 0.1, 0.2],\n","                       'max_depth': [-1, 10, 20]}\n","    },\n","    'MLP_Classifier': {\n","        'model': lambda: MLPClassifier(random_state=42),\n","        'params': {'random_state': 42},\n","        'param_grid': {\n","            'hidden_layer_sizes': [(100,), (50, 50), (100, 50, 25)],\n","            'activation': ['relu', 'tanh', 'logistic'],\n","            'solver': ['adam', 'sgd'],\n","            'alpha': [0.0001, 0.001, 0.01],\n","            'learning_rate': ['constant', 'adaptive']\n","        }\n","    },\n","    'Stacking': {\n","        'model': lambda: StackingClassifier(\n","            estimators=[(\"dt\", DecisionTreeClassifier(random_state=42)),\n","                        (\"knn\", KNeighborsClassifier(n_neighbors=5))],\n","            final_estimator=LogisticRegression(random_state=42, max_iter=1000)\n","        ),\n","        'params': {},\n","        'param_grid': {}\n","    },\n","    'Voting_Classifier': {\n","        'model': lambda: VotingClassifier(\n","            estimators=[(\"lr\", LogisticRegression(max_iter=1000, random_state=42)),\n","                        (\"dt\", DecisionTreeClassifier(random_state=42))],\n","            voting='soft', n_jobs=-1\n","        ),\n","        'params': {},\n","        'param_grid': {}\n","    },\n","    'ExtraTrees': {\n","        'model': ExtraTreesClassifier,\n","        'params': {'random_state': 42, 'n_estimators': 100},\n","        'param_grid': {'n_estimators': [50, 100, 200],\n","                       'max_depth': [None, 10, 20]}\n","    },\n","    'AdaBoost': {\n","        'model': AdaBoostClassifier,\n","        'params': {'random_state': 42, 'n_estimators': 50},\n","        'param_grid': {'n_estimators': [50, 100, 200],\n","                       'learning_rate': [0.01, 0.1, 1]}\n","    },\n","    'GradientBoosting': {\n","        'model': GradientBoostingClassifier,\n","        'params': {'random_state': 42},\n","        'param_grid': {'n_estimators': [50, 100, 200],\n","                       'learning_rate': [0.01, 0.1, 0.2],\n","                       'max_depth': [3, 5, 7]}\n","    },\n","    'Bagging_Classifier': {\n","        'model': BaggingClassifier,\n","        'params': {'random_state': 42},\n","        'param_grid': {'n_estimators': [10, 50, 100],\n","                       'max_samples': [0.5, 1.0],\n","                       'max_features': [0.5, 1.0],\n","                       'bootstrap': [True, False]}\n","    },\n","    'HistGradientBoosting': {\n","        'model': HistGradientBoostingClassifier,\n","        'params': {'random_state': 42},\n","        'param_grid': {'max_iter': [100, 200],\n","                       'learning_rate': [0.01, 0.1],\n","                       'max_depth': [3, 5, 10],\n","                       'min_samples_leaf': [20, 50]}\n","    },\n","    'NearestCentroid': {\n","        'model': NearestCentroid,\n","        'params': {'metric': 'euclidean'},\n","        'param_grid': {}\n","    },\n","    'ExtraTreesClassifier_Variant': {\n","        'model': ExtraTreesClassifier,\n","        'params': {'random_state': 42},\n","        'param_grid': {'n_estimators': [50, 100, 200],\n","                       'max_depth': [None, 10, 20],\n","                       'min_samples_split': [2, 5],\n","                       'min_samples_leaf': [1, 2]}\n","    },\n","    'Dummy_Classifier': {\n","        'model': DummyClassifier,\n","        'params': {},\n","        'param_grid': {'strategy': ['most_frequent', 'stratified', 'uniform', 'constant'],\n","                       'constant': [0, 1]}\n","    },\n","    'GaussianProcess': {\n","        'model': GaussianProcessClassifier,\n","        'params': {'random_state': 42},\n","        'param_grid': {'max_iter_predict': [100, 200],\n","                       'multi_class': ['one_vs_rest', 'one_vs_one']}\n","    },\n","    'ExtraTreesClassifier_New': {\n","        'model': ExtraTreesClassifier,\n","        'params': {'random_state': 42, 'n_estimators': 150},\n","        'param_grid': {'n_estimators': [100, 150, 200],\n","                       'max_depth': [None, 10, 20]}\n","    },\n","    'GaussianNB_Variant': {\n","        'model': GaussianNB,\n","        'params': {'priors': None},\n","        'param_grid': {}\n","    },\n","    'DecisionStump': {\n","        'model': DecisionTreeClassifier,\n","        'params': {'max_depth': 1, 'random_state': 42},\n","        'param_grid': {}\n","    },\n","    'LogisticRegressionCV': {\n","        'model': LogisticRegression,\n","        'params': {'max_iter': 1000, 'cv': 5, 'random_state': 42, 'class_weight': 'balanced'},\n","        'param_grid': {}\n","    },\n","    'SGDClassifier_Variant': {\n","        'model': SGDClassifier,\n","        'params': {'loss': 'hinge', 'max_iter': 1000, 'random_state': 42},\n","        'param_grid': {'alpha': [0.0001, 0.001, 0.01],\n","                       'penalty': ['l2', 'l1', 'elasticnet']}\n","    }\n","}\n","\n","# Define the feature selectors mapping using a dictionary.\n","INVOLVED_FEATURE_SELECTORS = {\n","    \"fs-Chi_Square_Test\": {\"function\": \"apply_chi_square\", \"params\": {}},\n","    \"fs-Correlation_Coefficient\": {\"function\": \"apply_correlation_coefficient\", \"params\": {}},\n","    \"fs-Mutual_Information\": {\"function\": \"apply_mutual_information\", \"params\": {}},\n","    \"fs-Variance_Threshold\": {\"function\": \"apply_variance_threshold\", \"params\": {}},\n","    \"fs-ANOVA_F_Test\": {\"function\": \"apply_anova_f_test\", \"params\": {}},\n","    \"fs-Information_Gain\": {\"function\": \"apply_information_gain\", \"params\": {}},\n","    \"fs-Univariate_Feature_Selection\": {\"function\": \"apply_univariate_feature_selection\", \"params\": {\"score_func\": mutual_info_classif}},\n","    \"fs-LASSO_FS\": {\"function\": \"apply_lasso_fs\", \"params\": {\"alpha\": 0.01}},\n","    \"fs-RFE\": {\"function\": \"apply_rfe_fs\", \"params\": {}},\n","    \"fs-ElasticNet_FS\": {\"function\": \"apply_elastic_net_fs\", \"params\": {\"alpha\": 0.01, \"l1_ratio\": 0.5}},\n","    \"fs-Sequential_FS\": {\"function\": \"apply_sequential_feature_selector_func\", \"params\": {\"direction\": \"forward\", \"scoring\": \"accuracy\", \"cv\": 3}},\n","    \"fs-FDR_FS\": {\"function\": \"apply_select_fdr\", \"params\": {\"score_func\": f_classif, \"alpha\": 0.05}},\n","    \"fs-FWE_FS\": {\"function\": \"apply_select_fwe\", \"params\": {\"score_func\": f_classif, \"alpha\": 0.05}},\n","    \"fs-Feature_Importance_RF\": {\"function\": \"apply_feature_importance_rf\", \"params\": {}},\n","    \"fs-Permutation_Importance\": {\"function\": \"apply_permutation_importance_fs\", \"params\": {\"scoring\": \"accuracy\"}},\n","    \"fs-ReliefF\": {\"function\": \"apply_relief_f\", \"params\": {\"n_features_to_select\": 10}},\n","    \"fs-Mutual_Info_Selection\": {\"function\": \"apply_mutual_info_selection\", \"params\": {}},\n","    \"fs-Select_From_Model_LR\": {\"function\": \"apply_select_from_model_lr\", \"params\": {}},\n","    \"fs-Extra_Trees_Importance\": {\"function\": \"apply_extra_trees_importance\", \"params\": {}},\n","    \"fs-Embedded_ElasticNet\": {\"function\": \"apply_embedded_elastic_net\", \"params\": {}},\n","    \"fs-VIF_Selection\": {\"function\": \"apply_vif_selection\", \"params\": {}},\n","    \"fs-Stability_Lasso\": {\"function\": \"apply_stability_lasso_selection\", \"params\": {\"n_runs\": 10, \"alphas\": [0.01]}},\n","    \"fs-Mutual_Info_Gain_Ratio\": {\"function\": \"apply_mutual_info_gain_ratio\", \"params\": {}},\n","    \"fs-Chi2_Pvalue\": {\"function\": \"apply_chi2_pvalue_selection\", \"params\": {}},\n","    \"fs-ANOVA_Pvalue\": {\"function\": \"apply_anova_pvalue_selection\", \"params\": {}},\n","    # Feature Extraction selectors\n","    \"fs-PCA_Loadings_Selection\": {\"function\": \"apply_pca_loadings_selection\", \"params\": {}},\n","    \"fs-PCA_Dictionary\": {\"function\": \"apply_pca_dictionary\", \"params\": {}},\n","    \"FE-PCA\": {\"function\": \"apply_pca\", \"params\": {}},\n","    \"FE-ICA\": {\"function\": \"apply_ica\", \"params\": {}},\n","    \"FE-t_SNE\": {\"function\": \"apply_t_sne\", \"params\": {}},\n","    \"FE-UMAP\": {\"function\": \"apply_umap_extraction\", \"params\": {}},\n","    \"FE-Kernel_PCA\": {\"function\": \"apply_kernel_pca_extraction\", \"params\": {}},\n","    \"FE-Truncated_SVD\": {\"function\": \"apply_truncated_svd_extraction\", \"params\": {}},\n","    \"FE-Feature_Agglomeration\": {\"function\": \"apply_feature_agglomeration_extraction\", \"params\": {}},\n","    \"FE-Isomap\": {\"function\": \"apply_isomap_extraction\", \"params\": {}},\n","    \"FE-Gaussian_Random_Projection\": {\"function\": \"apply_gaussian_random_projection_extraction\", \"params\": {}},\n","    \"FE-Autoencoder\": {\"function\": \"apply_autoencoder_extraction\", \"params\": {}},\n","    \"FE-Truncated_PCA\": {\"function\": \"apply_truncated_pca\", \"params\": {}},\n","    \"FE-Factor_Analysis\": {\"function\": \"apply_factor_analysis\", \"params\": {\"n_components\": 10}},\n","    \"FE-Sparse_PCA\": {\"function\": \"apply_sparse_pca\", \"params\": {\"n_components\": 10, \"alpha\": 1}},\n","    \"FE-NMF\": {\"function\": \"apply_nmf\", \"params\": {\"n_components\": 10, \"init\": \"random\", \"random_state\": RANDOM_SEED, \"max_iter\": 500}},\n","    \"FE-FastICA\": {\"function\": \"apply_fastica\", \"params\": {\"n_components\": 10, \"random_state\": RANDOM_SEED}},\n","    \"FE-Independent_Component_Analysis\": {\"function\": \"apply_independent_component_analysis\", \"params\": {}},\n","    \"FE-Locally_Linear_Embedding\": {\"function\": \"apply_locally_linear_embedding\", \"params\": {\"n_components\": 10}},\n","    \"FE-MDS\": {\"function\": \"apply_mds\", \"params\": {\"n_components\": 10, \"random_state\": RANDOM_SEED}},\n","    \"FE-Isomap_Alternative\": {\"function\": \"apply_isomap_alternative\", \"params\": {}},\n","    \"FE-Autoencoder_Deep\": {\"function\": \"apply_autoencoder_deep\", \"params\": {}},\n","    \"FE-Sparse_Random_Projection\": {\"function\": \"apply_sparse_random_projection_extraction\", \"params\": {}},\n","    \"FE-LDA_Extraction\": {\"function\": \"apply_lda_extraction\", \"params\": {}},\n","    \"FE-Spectral_Embedding\": {\"function\": \"apply_spectral_embedding_extraction\", \"params\": {}},\n","    \"FE-MDS_Different_Params\": {\"function\": \"apply_mds_different_params\", \"params\": {}},\n","    \"FE-t_SNE_Enhanced\": {\"function\": \"apply_t_sne_enhanced\", \"params\": {}},\n","    \"FE-Feature_Hashing\": {\"function\": \"apply_feature_hashing\", \"params\": {}},\n","    \"FE-UMAP_Min_Dist\": {\"function\": \"apply_umap_min_dist_extraction\", \"params\": {\"min_dist\": 0.1}},\n","    \"FE-Kernel_PCA_Poly\": {\"function\": \"apply_kernel_pca_poly_extraction\", \"params\": {\"degree\": 3}},\n","    \"FE-Diffusion_Map\": {\"function\": \"apply_diffusion_map_extraction\", \"params\": {}},\n","}\n","\n","# =============================================================================\n","# Start Processing Each Dataset\n","# =============================================================================\n","for dataset_file in dataset_files:\n","    try:\n","        # Print the current dataset processing status\n","        print(f\"\\n******** Processing Dataset: {dataset_file} ********\")\n","        # Build the full file path to the dataset file\n","        FILE_PATH = dataset_file\n","        # Extract dataset name by removing the file extension\n","        dataset_name = os.path.splitext(os.path.basename(dataset_file))[0]\n","        # Create a timestamp string for the results filename\n","        timestamp = datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n","        # Define the results Excel file path (inside a \"res\" folder)\n","        # Determine output folder: if user didn't set RESULTS_FOLDER, use default \"res\"\n","        output_dir = RESULTS_FOLDER if RESULTS_FOLDER else os.path.join(os.getcwd(), 'res')\n","        results_excel_path = os.path.join(output_dir, f\"results_{dataset_name}_{timestamp}.xlsx\")\n","        # Make sure the output directory exists\n","        os.makedirs(output_dir, exist_ok=True)\n","        # Create the directory for results if it does not exist\n","        if not os.path.exists(os.path.dirname(results_excel_path)):\n","            os.makedirs(os.path.dirname(results_excel_path))\n","\n","        # Create an empty initial Excel file to store results\n","        pd.DataFrame().to_excel(results_excel_path, index=False)\n","\n","        # Read data from file, supporting both Excel and CSV formats\n","        if FILE_PATH.endswith(('.xlsx', '.xls')):\n","            Org_Data = pd.read_excel(FILE_PATH, sheet_name=\"Sheet1\")\n","        elif FILE_PATH.endswith('.csv'):\n","            Org_Data = pd.read_csv(FILE_PATH)\n","        else:\n","            print(f\"File format of {dataset_file} is not supported. Skipping further processing.\")\n","            continue\n","        cols = Org_Data.columns.tolist()\n","        cols[0] = 'PatientID'\n","        cols[-1] = 'Outcome'\n","        Org_Data.columns = cols\n","\n","        # Assume that the outcome column is the last column in the dataset\n","        outcome_col = Org_Data.columns[-1]\n","        # Use the predefined ANALYSIS_MODE from the configuration\n","        if ANALYSIS_MODE == \"Semi-Supervised\":\n","            # In semi-supervised mode, fill missing Outcome values with the mode (most frequent value)\n","            try:\n","                mode_value = Org_Data[outcome_col].mode()[0]\n","                Org_Data[outcome_col].fillna(mode_value, inplace=True)\n","            except Exception as e:\n","                logging.error(f\"Error filling missing Outcome values: {e}\")\n","        elif ANALYSIS_MODE == \"Supervised\":\n","            # In supervised mode, drop rows where Outcome is missing\n","            Org_Data = Org_Data.dropna(subset=[outcome_col])\n","        else:\n","            raise ValueError(\"ANALYSIS_MODE must be 'Supervised' or 'Semi-Supervised'\")\n","\n","        # ----------------------------------------\n","        # Filter and sample class-wise data based on selection percent\n","        # ----------------------------------------\n","        filtered_data = pd.DataFrame()\n","\n","        for class_name, frac in CLASS_SELECTION_PERCENT.items():\n","            class_subset = Org_Data[Org_Data[outcome_col] == class_name]\n","            selected = class_subset.sample(frac=frac, random_state=RANDOM_SEED)\n","            filtered_data = pd.concat([filtered_data, selected], axis=0)\n","\n","        filtered_data = filtered_data.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n","\n","        # Ù†Ù…Ø§ÛŒØ´ ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ù‚Ø¨Ù„ Ùˆ Ø¨Ø¹Ø¯ Ø§Ø² ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù†\n","        show_class_distribution(Org_Data, filtered_data, outcome_col=outcome_col)\n","\n","        # Assume the first column is Patient_ID, the last column is Outcome, and the rest are features\n","        Patient_ID = filtered_data['PatientID']\n","        Data = filtered_data.iloc[:, 1:-1]\n","        Outcome = filtered_data.iloc[:, -1].map(CLASS_MAPPING)\n","        # Raise an error if any Outcome class could not be mapped\n","        if Outcome.isnull().any():\n","            raise ValueError(\"Some Outcome classes do not have a mapping.\")\n","\n","        # Select only numeric columns from the features\n","        numeric_columns = Data.select_dtypes(include=[np.number]).columns\n","        numeric_data = Data[numeric_columns]\n","\n","        # ---------------------------------------------------------------\n","        # Train/Test split â€“ based on TEST_MODE\n","        # ---------------------------------------------------------------\n","        if TEST_MODE == 'External':\n","            if not os.path.exists(EXTERNAL_TEST_FILE):\n","                raise FileNotFoundError(f\"External test file '{EXTERNAL_TEST_FILE}' not found.\")\n","\n","            ext_df = load_dataframe(EXTERNAL_TEST_FILE)\n","            cols = ext_df.columns.tolist()\n","            cols[0] = 'PatientID'\n","            cols[-1] = 'Outcome'\n","            ext_df.columns = cols\n","\n","            X_test = ext_df.reindex(columns=numeric_columns, fill_value=np.nan)\n","            y_test = ext_df['Outcome'].map(CLASS_MAPPING)\n","            Patient_ID_test = ext_df['PatientID']\n","\n","            X_train = numeric_data\n","            y_train = Outcome\n","            Patient_ID_train = Patient_ID\n","\n","        else:  # Internal test mode\n","            X_train, X_test, y_train, y_test, Patient_ID_train, Patient_ID_test = train_test_split(\n","                numeric_data, Outcome, Patient_ID,\n","                test_size=TEST_SIZE, stratify=Outcome, random_state=RANDOM_SEED\n","            )\n","\n","        # Imputation\n","        imputer = SimpleImputer(strategy=IMPUTATION_STRATEGY)\n","        X_train_imputed = imputer.fit_transform(X_train)\n","        X_test_imputed  = imputer.transform(X_test)\n","\n","        if SCALING_METHOD == 'MinMaxScaler':\n","            scaler = MinMaxScaler()\n","        elif SCALING_METHOD == 'StandardScaler':\n","            scaler = StandardScaler()\n","        elif SCALING_METHOD == 'RobustScaler':\n","            scaler = RobustScaler()\n","        elif SCALING_METHOD == 'Normalizer':\n","            scaler = Normalizer()\n","        elif SCALING_METHOD == 'MaxAbsScaler':\n","            scaler = MaxAbsScaler()\n","        else:\n","            scaler = None\n","\n","        if scaler is not None:\n","            X_train_scaled = scaler.fit_transform(X_train_imputed)\n","            X_test_scaled  = scaler.transform(X_test_imputed)\n","        else:\n","            X_train_scaled = X_train_imputed\n","            X_test_scaled  = X_test_imputed\n","\n","\n","        # Save a summary of the preprocessing into the results file in a \"Preprocessing\" sheet\n","        preproc_df = pd.DataFrame({\n","            \"Num Samples\": [len(Outcome)],\n","            \"Unique Outcomes\": [', '.join(map(str, Outcome.unique()))],\n","            \"Analysis Mode\": [ANALYSIS_MODE]\n","        })\n","        preproc_df.to_excel(results_excel_path, sheet_name=\"Preprocessing\", index=False)\n","\n","        # Create a StratifiedKFold for cross-validation\n","        skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n","\n","        # Lists to store selected features and best parameters for later reporting\n","        selected_features_all = []\n","        best_parameters_all = []\n","        # Loop over Outcome classifiers (only those selected in the OUTCOME_CLASSIFIERS list)\n","        # Skip training Outcome Classifier if in 'Supervised' mode\n","       # Outcome Classifier Logic: Only used in Semi-Supervised mode\n","        # === Outcome Classifier Phase ===\n","        if ANALYSIS_MODE == \"Supervised\":\n","            logging.info(\"=== ANALYSIS_MODE is 'Supervised': Outcome classifier skipped. ===\")\n","\n","            # ðŸ§¼ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª Ú©Ù‡ outcome Ù†Ø¯Ø§Ø±Ù†\n","            test_mask = ~y_test.isnull()\n","            X_test = X_test[test_mask]\n","            X_test_scaled = X_test_scaled[test_mask]\n","            y_test = y_test[test_mask]\n","            Patient_ID_test = Patient_ID_test[test_mask]\n","\n","            # outcomeÙ‡Ø§ Ù‡Ù…ÙˆÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ù‡Ø³ØªÙ†ØŒ Ù†Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡\n","            y_train_pred = y_train.copy()\n","            y_test_pred = y_test.copy()\n","\n","            outcome_clf_names = [\"Supervised_Mode\"]\n","\n","        elif ANALYSIS_MODE == \"Semi-Supervised\":\n","            outcome_clf_names = OUTCOME_CLASSIFIERS\n","            for outcome_clf_name in outcome_clf_names:\n","                print(f\"\\n=== Outcome Classifier: {outcome_clf_name} ===\")\n","                oc_info = INVOLVED_CLASSIFIERS.get(outcome_clf_name)\n","                if oc_info is None:\n","                    print(f\"Classifier {outcome_clf_name} not found.\")\n","                    continue\n","\n","                try:\n","                    oc_model = oc_info['model'](**oc_info['params'])\n","                    oc_model.fit(X_train_scaled, y_train)\n","\n","                    ext_df = load_dataframe(EXTERNAL_TEST_FILE)\n","                    ext_feats = ext_df.reindex(columns=numeric_columns, fill_value=np.nan)\n","                    ext_imp = imputer.transform(ext_feats)\n","                    ext_scaled = scaler.transform(ext_imp) if scaler is not None else ext_imp\n","\n","                    y_orig = ext_df['Outcome'].map(CLASS_MAPPING)\n","                    missing_mask = y_orig.isna()\n","                    y_pred = oc_model.predict(ext_scaled[missing_mask])\n","                    y_test = y_orig.copy()\n","                    y_test.loc[missing_mask] = y_pred\n","\n","                    X_test = ext_feats\n","                    X_test_scaled = ext_scaled\n","                    Patient_ID_test = ext_df['PatientID']\n","\n","                    y_train_pred = y_train.copy()\n","                    y_test_pred = y_test.copy()\n","\n","                except Exception as e:\n","                    print(f\"Error training Outcome Classifier {outcome_clf_name}: {e}\")\n","                    continue\n","        else:\n","            raise ValueError(\"ANALYSIS_MODE must be either 'Supervised' or 'Semi-Supervised'\")\n","\n","\n","        for outcome_clf_name in outcome_clf_names:\n","\n","\n","            # Record best parameters for each feature selector and analysis classifier\n","            best_params_record = {}\n","            # Loop through each feature selector\n","            for fs_name in INVOLVED_FEATURE_SELECTORS.keys():\n","                fs_info = INVOLVED_FEATURE_SELECTORS[fs_name]\n","                print(f\"\\n--- Running Feature Selector: {fs_name} for Outcome {outcome_clf_name} ---\")\n","                fs_func_name = fs_info['function']\n","                fs_params = fs_info['params']\n","                # Fetch the corresponding function object from the mapping dictionary\n","                fs_function = feature_selector_functions.get(fs_func_name)\n","                if fs_function is None:\n","                    print(f\"Function '{fs_func_name}' for {fs_name} not found.\")\n","                    continue\n","\n","                try:\n","                    # Use the predicted Outcome as the label for feature selection\n","                    X_train_selected, selector_obj = fs_function(X_train_scaled, y_train_pred, NOF, **fs_params)\n","                except Exception as e:\n","                    print(f\"Error in Feature Selector {fs_name}: {e}.\")\n","                    continue\n","\n","                # Extract the names of the selected features\n","                sel_feature_names = []\n","                if selector_obj is not None:\n","                    if hasattr(selector_obj, 'get_support'):\n","                        sel_feature_names = numeric_columns[selector_obj.get_support()].tolist()\n","                    elif isinstance(selector_obj, (np.ndarray, list)):\n","                        sel_feature_names = numeric_columns.take(selector_obj).tolist()\n","                    else:\n","                        sel_feature_names = numeric_columns.tolist()\n","                else:\n","                    sel_feature_names = numeric_columns.tolist()\n","\n","                sel_feature_names = [str(f) for f in sel_feature_names]\n","                selected_features_all.append({\n","                    'Outcome Classifier': outcome_clf_name,\n","                    'Feature Selector': fs_name,\n","                    'Selected Features': ', '.join(sel_feature_names)\n","                })\n","                append_df_to_excel(\n","                    results_excel_path,\n","                    pd.DataFrame([{\n","                        'Outcome Classifier': outcome_clf_name,\n","                        'Feature Selector': fs_name,\n","                        'Selected Features': ', '.join(sel_feature_names)\n","                    }]),\n","                    sheet_name=\"Selected_Features\"\n","                )\n","\n","                # Extract the selected features from the test set using the same selector object\n","                if selector_obj is not None:\n","                    if hasattr(selector_obj, 'transform'):\n","                        try:\n","                            X_test_selected = selector_obj.transform(X_test_scaled)\n","                        except:\n","                            X_test_selected = X_test_scaled\n","                    elif isinstance(selector_obj, (np.ndarray, list)):\n","                        X_test_selected = X_test_scaled[:, selector_obj]\n","                    else:\n","                        X_test_selected = X_test_scaled\n","                else:\n","                    X_test_selected = X_test_scaled\n","\n","                # Loop over each analysis classifier defined in the INVOLVED_CLASSIFIERS dictionary\n","                for clf_name in INVOLVED_CLASSIFIERS.keys():\n","                    print(f\"\\n--- Grid Search for Analysis Classifier {clf_name} with Feature Selector {fs_name} for Outcome {outcome_clf_name} ---\")\n","                    print(f\"Classifier_Outcom: {outcome_clf_name}  |  Classifier_Analysis: {clf_name}\")\n","                    clf_info = INVOLVED_CLASSIFIERS[clf_name]\n","                    model_class = clf_info['model']\n","                    model_initial_params = clf_info['params'].copy()\n","                    param_grid = clf_info.get('param_grid', {})\n","\n","                    # Build the initial classifier model\n","                    try:\n","                        if callable(model_class):\n","                            model = model_class()\n","                            for k, v in model_initial_params.items():\n","                                setattr(model, k, v)\n","                        else:\n","                            model = model_class(**model_initial_params)\n","                    except Exception as e:\n","                        print(f\"Error initializing classifier {clf_name}: {e}.\")\n","                        continue\n","\n","                    # Use RandomizedSearchCV to find the best hyperparameters\n","                    try:\n","                        search = RandomizedSearchCV(\n","                            estimator=model,\n","                            param_distributions=param_grid,\n","                            n_iter=5,\n","                            cv=3,\n","                            scoring='accuracy',\n","                            random_state=RANDOM_SEED,\n","                            n_jobs=-1,\n","                            verbose=1\n","                        )\n","                        search.fit(X_train_selected, y_train_pred)\n","                        best_params = search.best_params_\n","                        best_score = search.best_score_\n","                        print(f\"Best parameters for {clf_name}: {best_params} (CV Accuracy: {best_score:.4f})\")\n","                    except Exception as e:\n","                        print(f\"Grid search for {clf_name} encountered an error: {e}. Using initial parameters.\")\n","                        best_params = model_initial_params\n","\n","                    # Record the best parameters found during hyperparameter search\n","                    best_params_record.setdefault(fs_name, {})[clf_name] = best_params\n","                    best_params_df = pd.DataFrame([{\n","                        'Outcome Classifier': outcome_clf_name,\n","                        'Feature Selector': fs_name,\n","                        'Classifier': clf_name,\n","                        **best_params\n","                    }])\n","                    append_df_to_excel(results_excel_path, best_params_df, sheet_name=\"Best_Parameters\")\n","\n","                    # Prepare for cross-validation of the current analysis classifier with the selected features\n","                    fold_results = []\n","                    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_selected, y_train), start=1):\n","\n","                        try:\n","                            if callable(model_class):\n","                                model_fold = model_class()\n","                                for k, v in {**model_initial_params, **best_params}.items():\n","                                    setattr(model_fold, k, v)\n","                            else:\n","                                model_fold = model_class(**{**model_initial_params, **best_params})\n","                        except Exception as e:\n","                            print(f\"Error initializing model {clf_name} in Fold {fold}: {e}.\")\n","                            continue\n","\n","                        try:\n","                            X_train_fold, X_val_fold = X_train_selected[train_idx], X_train_selected[val_idx]\n","                            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n","                        except Exception as e:\n","                            print(f\"Error splitting data in Fold {fold}: {e}\")\n","                            continue\n","\n","                        try:\n","                            model_fold.fit(X_train_fold, y_train_fold)\n","                        except Exception as e:\n","                            print(f\"Error training {clf_name} in Fold {fold}: {e}.\")\n","                            continue\n","\n","                        # Compute evaluation metrics for the validation fold\n","                        try:\n","                            y_val_pred = model_fold.predict(X_val_fold)\n","                            val_accuracy = accuracy_score(y_val_fold, y_val_pred)\n","                            val_precision = precision_score(y_val_fold, y_val_pred, average='weighted', zero_division=0)\n","                            val_recall = recall_score(y_val_fold, y_val_pred, average='weighted', zero_division=0)\n","                            val_f1 = f1_score(y_val_fold, y_val_pred, average='weighted', zero_division=0)\n","                            val_auc, val_specificity = compute_auc_and_specificity(model_fold, X_val_fold, y_val_fold)\n","                        except Exception as e:\n","                            print(f\"Error computing validation metrics in Fold {fold} for {clf_name}: {e}\")\n","                            val_accuracy = np.nan\n","                            val_precision = np.nan\n","                            val_recall = np.nan\n","                            val_f1 = np.nan\n","                            val_auc = np.nan\n","                            val_specificity = np.nan\n","                        try:\n","                            y_test_pred_fold = model_fold.predict(X_test_selected)\n","\n","                            # compare against TRUE test labels\n","                            test_accuracy  = accuracy_score(y_test, y_test_pred_fold)\n","                            test_precision = precision_score(y_test, y_test_pred_fold, average='weighted', zero_division=0)\n","                            test_recall    = recall_score(y_test, y_test_pred_fold, average='weighted', zero_division=0)\n","                            test_f1        = f1_score(y_test, y_test_pred_fold, average='weighted', zero_division=0)\n","                            test_auc, test_specificity = compute_auc_and_specificity(\n","                                model_fold, X_test_selected, y_test\n","                            )\n","                        except Exception as e:\n","                            print(f\"Error computing test metrics in Fold {fold} for {clf_name}: {e}\")\n","                            test_accuracy = np.nan\n","                            test_precision = np.nan\n","                            test_recall = np.nan\n","                            test_f1 = np.nan\n","                            test_auc = np.nan\n","                            test_specificity = np.nan\n","\n","\n","                        # Save fold results in a dictionary\n","                        result_dict = {\n","                            'Outcome Classifier': outcome_clf_name,\n","                            'Feature Selector': fs_name,\n","                            'Classifier_Analysis': clf_name,\n","                            'CV_Fold': fold,\n","                            'Validation_Accuracy': val_accuracy,\n","                            'Validation_Precision': val_precision,\n","                            'Validation_Recall': val_recall,\n","                            'Validation_F1': val_f1,\n","                            'Validation_AUC': val_auc,\n","                            'Validation_Specificity': val_specificity,\n","                            'Test_Accuracy': test_accuracy,\n","                            'Test_Precision': test_precision,\n","                            'Test_Recall': test_recall,\n","                            'Test_F1': test_f1,\n","                            'Test_AUC': test_auc,\n","                            'Test_Specificity': test_specificity\n","                        }\n","                        fold_results.append(result_dict)\n","\n","                    # Convert the fold results to a DataFrame and append to the Excel results file\n","                    df_fold_results = pd.DataFrame(fold_results)\n","                    append_df_to_excel(results_excel_path, df_fold_results, sheet_name=\"CV_Results\")\n","\n","                    # Compute the mean metrics across folds and append these aggregated results\n","                    if not df_fold_results.empty:\n","                        metrics_cols = [\n","                            'Validation_Accuracy', 'Validation_Precision', 'Validation_Recall', 'Validation_F1',\n","                            'Validation_AUC', 'Validation_Specificity', 'Test_Accuracy', 'Test_Precision',\n","                            'Test_Recall', 'Test_F1', 'Test_AUC', 'Test_Specificity'\n","                        ]\n","                        aggregated_mean = df_fold_results[metrics_cols].mean().to_dict()\n","                        aggregated_mean['Outcome Classifier'] = outcome_clf_name\n","                        aggregated_mean['Feature Selector'] = fs_name\n","                        aggregated_mean['Classifier_Analysis'] = clf_name\n","                        append_df_to_excel(results_excel_path, pd.DataFrame([aggregated_mean]), sheet_name=\"Aggregated_Results\")\n","\n","                        aggregated_std = df_fold_results[metrics_cols].std().to_dict()\n","                        aggregated_std['Outcome Classifier'] = outcome_clf_name\n","                        aggregated_std['Feature Selector'] = fs_name\n","                        aggregated_std['Classifier_Analysis'] = clf_name\n","\n","                        append_df_to_excel(results_excel_path, pd.DataFrame([aggregated_std]), sheet_name=\"Aggregated_Std_Results\")\n","\n","\n","            best_parameters_all.append({\n","                'Outcome Classifier': outcome_clf_name,\n","                'Best Parameters': best_params_record\n","            })\n","\n","        # Save the final selected features into the results file\n","        df_selected_features = pd.DataFrame(selected_features_all)\n","        append_df_to_excel(results_excel_path, df_selected_features, sheet_name=\"Selected_Features\")\n","\n","        # Save the best parameters for each classifier and selector combination\n","        best_params_records = []\n","        for record in best_parameters_all:\n","            oc = record['Outcome Classifier']\n","            for fs, clf_dict in record['Best Parameters'].items():\n","                for clf, params in clf_dict.items():\n","                    flat_params = {f\"{clf}_{k}\": v for k, v in params.items()}\n","                    flat_params['Outcome Classifier'] = oc\n","                    flat_params['Feature Selector'] = fs\n","                    flat_params['Classifier'] = clf\n","                    best_params_records.append(flat_params)\n","        df_best_params = pd.DataFrame(best_params_records)\n","        append_df_to_excel(results_excel_path, df_best_params, sheet_name=\"Best_Parameters\")\n","\n","        # Copy the original dataset file next to the results file for reference.\n","        original_data_filename = os.path.basename(FILE_PATH)\n","        destination_path = os.path.join(os.path.dirname(results_excel_path), f\"Original_{original_data_filename}\")\n","        shutil.copyfile(FILE_PATH, destination_path)\n","\n","    except Exception as e:\n","        print(f\"âŒ Error processing dataset {dataset_file}: {e}\")\n","        traceback.print_exc()\n","        continue\n","\n","print(\"Pipeline execution completed.\")\n"]}],"metadata":{"colab":{"provenance":[]},"kernel_info":{"name":"python310-sdkv2"},"kernelspec":{"display_name":"Python 3.11.3 (jupyter_env)","language":"python","name":"jupyter_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"microsoft":{"host":{"AzureML":{"notebookHasBeenCompleted":true}},"ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":0}